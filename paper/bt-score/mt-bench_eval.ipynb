{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT-Bench BT Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xy/.pyenv/versions/3.12.11/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'mt-bench' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Generating train split: 5755 examples [00:00, 297261.39 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question_id', 'model_a', 'model_b', 'winner', 'judge', 'conversation_a', 'conversation_b', 'turn'],\n",
      "        num_rows: 5755\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'ERROR'\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# mt_bench = load_dataset('/nas/data/mt-bench', trust_remote_code=True)\n",
    "mt_bench = load_dataset('mt-bench', trust_remote_code=True)\n",
    "models = np.unique(mt_bench['train']['model_a'])\n",
    "print(mt_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(mt_bench['train'])\n",
    "# df.to_csv('mt-bench.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://datascience.oneoffcoder.com/btl-model.html\n",
    "def get_estimate(i, p, df):\n",
    "    get_prob = lambda i, j: np.nan if i == j else p.iloc[i] + p.iloc[j]\n",
    "    n = df.iloc[i].sum()\n",
    "\n",
    "    d_n = df.iloc[i] + df.iloc[:, i]\n",
    "    d_d = pd.Series([get_prob(i, j) for j in range(len(p))], index=p.index)\n",
    "    d = (d_n / d_d).sum()\n",
    "\n",
    "    return n / d\n",
    "\n",
    "def estimate_p(p, df):\n",
    "    return pd.Series([get_estimate(i, p, df) for i in range(df.shape[0])], index=p.index)\n",
    "\n",
    "\n",
    "def iterate(df, p=None, n=20, sorted=True):\n",
    "    if p is None:\n",
    "        p = pd.Series([1 for _ in range(df.shape[0])], index=list(df.columns))\n",
    "\n",
    "    estimates = [p]\n",
    "\n",
    "    for _ in range(n):\n",
    "        p = estimate_p(p, df)\n",
    "        p = p / p.sum()\n",
    "        estimates.append(p)\n",
    "\n",
    "    p = p.sort_values(ascending=False) if sorted else p\n",
    "    return p, pd.DataFrame(estimates)\n",
    "\n",
    "win_count = {model_a: {model_b: 0 for model_b in models} for model_a in models}\n",
    "for line in mt_bench['train']:\n",
    "    if line['winner'] == 'model_a':\n",
    "        win_count[line['model_b']][line['model_a']] += 1\n",
    "    elif line['winner'] == 'model_b':\n",
    "        win_count[line['model_a']][line['model_b']] += 1\n",
    "df = pd.DataFrame(win_count)\n",
    "p, estimates = iterate(df, n=100)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT-Bench FACE Score BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face import spectrum_pipeline, evaluate_pipeline\n",
    "from inference import inference\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "\n",
    "est_path = '/data1/model/pythia-1_4b-base'\n",
    "inference_model_name = est_path.split('/')[-1]\n",
    "gpu_mem = 0.9\n",
    "\n",
    "metrics = ['so', 'corr', 'spear', 'emd', 'kl', 'js']\n",
    "\n",
    "schema = pa.schema({\n",
    "    'input': pa.string(),\n",
    "    'output': pa.string()\n",
    "})\n",
    "\n",
    "llm = LLM(\n",
    "    est_path,\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=2048,\n",
    "    tensor_parallel_size=torch.cuda.device_count()\n",
    ")\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0, \n",
    "    prompt_logprobs=0, \n",
    "    max_tokens=1\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(est_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_counts = {\n",
    "    'so': {model_a: {model_b: 0 for model_b in models} for model_a in models},\n",
    "    'corr': {model_a: {model_b: 0 for model_b in models} for model_a in models},\n",
    "    'spear': {model_a: {model_b: 0 for model_b in models} for model_a in models},\n",
    "    'emd': {model_a: {model_b: 0 for model_b in models} for model_a in models},\n",
    "    'kl': {model_a: {model_b: 0 for model_b in models} for model_a in models},\n",
    "    'js': {model_a: {model_b: 0 for model_b in models} for model_a in models}\n",
    "}\n",
    "\n",
    "gpt4_dict = {}\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    if line['model_a'] == 'gpt-4':\n",
    "        for text in line['conversation_a']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "    elif line['model_b'] == 'gpt-4':\n",
    "        for text in line['conversation_b']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "# for idx, (key, value) in enumerate(gpt4_dict.items()):\n",
    "#     print(key)\n",
    "#     print('=' * 10)\n",
    "#     print(value)\n",
    "#     if idx == 3:\n",
    "#         break\n",
    "# print(len(gpt4_dict))\n",
    "\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    def eval_line(conversation_name):\n",
    "        input = []\n",
    "        output = []\n",
    "        gpt4_output = []\n",
    "        for text in line[conversation_name]:\n",
    "            if text['role'] == 'user':\n",
    "                text_input = text['content']\n",
    "            else:\n",
    "                if text_input != \"\" and text['content'] != \"\":\n",
    "                    input.extend([text_input] * len(gpt4_dict[text_input]))\n",
    "                    output.extend([text['content']] * len(gpt4_dict[text_input]))\n",
    "                    gpt4_output.extend(gpt4_dict[text_input])\n",
    "        data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        data = Dataset(data)\n",
    "        inferenced_data = inference(llm, sampling_params, data, tokenizer, False)\n",
    "        model_spectrum = spectrum_pipeline(inferenced_data['logprobs'])\n",
    "\n",
    "        data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, gpt4_output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        data = Dataset(data)\n",
    "        inferenced_data = inference(llm, sampling_params, data, tokenizer, False)\n",
    "        human_spectrum = spectrum_pipeline(inferenced_data['logprobs'])\n",
    "        raw_results = evaluate_pipeline(human_spectrum, model_spectrum, metrics)\n",
    "        return_results = []\n",
    "        last_input = \"\"\n",
    "        for idx, line_result in enumerate(raw_results):\n",
    "            line_input = input[idx]\n",
    "            if line_input != last_input:\n",
    "                if last_input != \"\":\n",
    "                    line_return_result = {}\n",
    "                    for metric in metrics:\n",
    "                        line_return_result[metric] = np.max(group_results[metric]) if metric in ['so', 'corr', 'spear'] else np.min(group_results[metric])\n",
    "                    return_results.append(line_return_result)\n",
    "                group_results = {metric: [] for metric in metrics}\n",
    "                last_input = line_input\n",
    "            else:\n",
    "                for metric in metrics:\n",
    "                    group_results[metric].append(line_result[metric])\n",
    "        return return_results\n",
    "\n",
    "    result_a = eval_line('conversation_a')\n",
    "    result_b = eval_line('conversation_b')\n",
    "    for metric in metrics:\n",
    "        sum = 0\n",
    "        for result in result_a:\n",
    "            sum += result[metric]\n",
    "        mean_result_a = sum / len(result_a)\n",
    "        sum = 0\n",
    "        for result in result_b:\n",
    "            sum += result[metric]\n",
    "        mean_result_b = sum / len(result_b)\n",
    "        # greater better\n",
    "        if metric in ['so', 'corr', 'spear']:\n",
    "            if mean_result_a > mean_result_b:\n",
    "                win_counts[metric][line['model_a']][line['model_b']] += 1\n",
    "            elif mean_result_a < mean_result_b:\n",
    "                win_counts[metric][line['model_b']][line['model_a']] += 1\n",
    "        else:\n",
    "            if mean_result_a < mean_result_b:\n",
    "                win_counts[metric][line['model_a']][line['model_b']] += 1\n",
    "            elif mean_result_a > mean_result_b:\n",
    "                win_counts[metric][line['model_b']][line['model_a']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in win_counts.items():\n",
    "    print(key)\n",
    "    print('-' * 10)\n",
    "    df = pd.DataFrame(value)\n",
    "    p, estimates = iterate(df, n=100)\n",
    "    print(p)\n",
    "    print('=' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT-Bench MAUVE Score BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mauve\n",
    "mauve_win_count = {model_a: {model_b: 0 for model_b in models} for model_a in models}\n",
    "\n",
    "def mauve_pipeline(human_data, model_data):\n",
    "    predictions = [x['input'] + x['output'] for x in model_data]\n",
    "    references = [x['input'] + x['output'] for x in human_data]\n",
    "    return mauve.compute_mauve(p_text=predictions, q_text=references, featurize_model_name=est_path, verbose=False)\n",
    "\n",
    "gpt4_dict = {}\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    if line['model_a'] == 'gpt-4':\n",
    "        for text in line['conversation_a']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "    elif line['model_b'] == 'gpt-4':\n",
    "        for text in line['conversation_b']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    def eval_line(conversation_name):\n",
    "        input = []\n",
    "        output = []\n",
    "        gpt4_output = []\n",
    "        for text in line[conversation_name]:\n",
    "            if text['role'] == 'user':\n",
    "                text_input = text['content']\n",
    "            else:\n",
    "                if text_input != \"\" and text['content'] != \"\":\n",
    "                    # input.extend([text_input] * len(gpt4_dict[text_input]))\n",
    "                    # output.extend([text['content']] * len(gpt4_dict[text_input]))\n",
    "                    # gpt4_output.extend(gpt4_dict[text_input])\n",
    "\n",
    "                    input.append(text_input)\n",
    "                    output.append(text['content'])\n",
    "                    gpt4_output.append(gpt4_dict[text_input][0])\n",
    "        model_data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        model_data = Dataset(model_data)\n",
    "\n",
    "        human_data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, gpt4_output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        human_data = Dataset(human_data)\n",
    "        raw_results = mauve_pipeline(human_data, model_data)\n",
    "\n",
    "        return raw_results.mauve\n",
    "        # return_results = []\n",
    "        # last_input = \"\"\n",
    "        # for idx, line_result in enumerate(raw_results):\n",
    "        #     line_input = input[idx]\n",
    "        #     if line_input != last_input:\n",
    "        #         if last_input != \"\":\n",
    "        #             line_return_result = np.max(group_results)\n",
    "        #             return_results.append(line_return_result)\n",
    "        #         group_results = []\n",
    "        #         last_input = line_input\n",
    "        #     else:\n",
    "        #         group_results.append(line_result['mauve'])\n",
    "        # return return_results\n",
    "\n",
    "    result_a = eval_line('conversation_a')\n",
    "    result_b = eval_line('conversation_b')\n",
    "\n",
    "    mean_result_a = result_a\n",
    "    mean_result_b = result_b\n",
    "    # sum = 0\n",
    "    # for result in result_a:\n",
    "    #     sum += result\n",
    "    # mean_result_a = sum / len(result_a)\n",
    "    # sum = 0\n",
    "    # for result in result_b:\n",
    "    #     sum += result\n",
    "    # mean_result_b = sum / len(result_b)\n",
    "    # greater better\n",
    "    if mean_result_a > mean_result_b:\n",
    "        mauve_win_count[line['model_b']][line['model_a']] += 1\n",
    "    elif mean_result_a < mean_result_b:\n",
    "        mauve_win_count[line['model_a']][line['model_b']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mauve\")\n",
    "print('-' * 10)\n",
    "df = pd.DataFrame(mauve_win_count)\n",
    "p, estimates = iterate(df, n=100)\n",
    "print(p)\n",
    "print('=' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT-Bench FACE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from inference import inference\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "\n",
    "est_path = '/data1/model/pythia-1_4b-base'\n",
    "inference_model_name = est_path.split('/')[-1]\n",
    "gpu_mem = 0.9\n",
    "\n",
    "schema = pa.schema({\n",
    "    'input': pa.string(),\n",
    "    'output': pa.string()\n",
    "})\n",
    "# model_generations = {model: set() for model in models}\n",
    "# for line in mt_bench['train']:\n",
    "#     for content in line['conversation_a']:\n",
    "#         model_generations[line['model_a']].add(content['content'])\n",
    "#     for content in line['conversation_b']:\n",
    "#         model_generations[line['model_b']].add(content['content'])\n",
    "\n",
    "generation_pairs = {model: [] for model in models}\n",
    "for line in mt_bench['train']:\n",
    "    if line['model_a'] == 'gpt-4':\n",
    "        for text_pair in zip(line['conversation_a'], line['conversation_b']):\n",
    "            generation_pairs[line['model_b']].append((text_pair[0], text_pair[1]))\n",
    "    elif line['model_b'] == 'gpt-4':\n",
    "        for text_pair in zip(line['conversation_b'], line['conversation_a']):\n",
    "            generation_pairs[line['model_a']].append((text_pair[0], text_pair[1]))\n",
    "\n",
    "llm = LLM(\n",
    "    est_path,\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=2048,\n",
    "    tensor_parallel_size=torch.cuda.device_count()\n",
    ")\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0, \n",
    "    prompt_logprobs=0, \n",
    "    max_tokens=1\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(est_path)\n",
    "\n",
    "for model_name in tqdm(models):\n",
    "    input = []\n",
    "    output = []\n",
    "    for text_pair in generation_pairs[model_name]:\n",
    "        if text_pair[1]['role'] == 'user':\n",
    "            input.append(text_pair[1]['content'])\n",
    "        else:\n",
    "            output.append(text_pair[1]['content'])\n",
    "    data = pa.Table.from_pydict(\n",
    "        dict(\n",
    "            zip(schema.names, (input, output))\n",
    "        ),\n",
    "        schema=schema\n",
    "    )\n",
    "    data = Dataset(data)\n",
    "    data = data.filter(lambda x: x['input'] != '' and x['output'] != '')\n",
    "    inferenced_data = inference(llm, sampling_params, data, tokenizer)\n",
    "    inferenced_data.save_to_disk(f'./inference/{inference_model_name}-inferenced.{model_name}.{'mt-bench'}')\n",
    "\n",
    "    input = []\n",
    "    output = []\n",
    "    for text_pair in generation_pairs[model_name]:\n",
    "        if text_pair[0]['role'] == 'user':\n",
    "            input.append(text_pair[0]['content'])\n",
    "        else:\n",
    "            output.append(text_pair[0]['content'])\n",
    "    data = pa.Table.from_pydict(\n",
    "        dict(\n",
    "            zip(schema.names, (input, output))\n",
    "        ),\n",
    "        schema=schema\n",
    "    )\n",
    "    data = Dataset(data)\n",
    "    data = data.filter(lambda x: x['input'] != '' and x['output'] != '')\n",
    "    inferenced_data = inference(llm, sampling_params, data, tokenizer)\n",
    "    inferenced_data.save_to_disk(f'./inference/{inference_model_name}-inferenced.{model_name+'-gpt4'}.{'mt-bench'}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# run face.py first\n",
    "eval_path = './eval/real-en/raw'\n",
    "scores = {}\n",
    "models = []\n",
    "for file in os.listdir(eval_path):\n",
    "    model = file.split('.')[1]\n",
    "    models.append(model)\n",
    "    with open(os.path.join(eval_path, file))as f:\n",
    "        jsonl = [json.loads(line) for line in f][0]\n",
    "        so = np.mean([jsonl[idx]['so'] for idx in range(len(jsonl))])\n",
    "        corr = np.mean([jsonl[idx]['corr'] for idx in range(len(jsonl))])\n",
    "        spear = np.mean([jsonl[idx]['spear'] for idx in range(len(jsonl))])\n",
    "        emd = np.mean([jsonl[idx]['emd'] for idx in range(len(jsonl))])\n",
    "        kl = np.mean([jsonl[idx]['kl'] for idx in range(len(jsonl))])\n",
    "        js = np.mean([jsonl[idx]['js'] for idx in range(len(jsonl))])\n",
    "    scores[model] = {\n",
    "        'so': so,\n",
    "        'corr': corr,\n",
    "        'spear': spear,\n",
    "        'emd': emd,\n",
    "        'kl': kl,\n",
    "        'js':js\n",
    "    }\n",
    "\n",
    "for model in models:\n",
    "    print(f'{model:13s} ||', end='')\n",
    "    for key, value in scores[model].items():\n",
    "        print(f' {key}: {value:.4f} |', end='')\n",
    "    print()\n",
    "\n",
    "metrics = ['so', 'corr', 'spear', 'emd', 'kl', 'js']\n",
    "for metric in metrics:\n",
    "    pairs = []\n",
    "    for model in models:\n",
    "        pairs.append((model, scores[model][metric]))\n",
    "    sorted_list = sorted(pairs, key=lambda x: x[1], reverse=True if metric in ['so', 'corr', 'spear'] else False)\n",
    "    print(metric)\n",
    "    print('-' * 10)\n",
    "    for item in sorted_list:\n",
    "        print(f'{item[0]:13s} | {item[1]:.4f}')\n",
    "    print('=' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT-Bench MAUVE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from inference import inference\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "\n",
    "est_path = '/data1/model/pythia-1_4b-base'\n",
    "inference_model_name = est_path.split('/')[-1]\n",
    "gpu_mem = 0.9\n",
    "\n",
    "schema = pa.schema({\n",
    "    'input': pa.string(),\n",
    "    'output': pa.string()\n",
    "})\n",
    "# model_generations = {model: set() for model in models}\n",
    "# for line in mt_bench['train']:\n",
    "#     for content in line['conversation_a']:\n",
    "#         model_generations[line['model_a']].add(content['content'])\n",
    "#     for content in line['conversation_b']:\n",
    "#         model_generations[line['model_b']].add(content['content'])\n",
    "\n",
    "generation_pairs = {model: [] for model in models}\n",
    "for line in mt_bench['train']:\n",
    "    if line['model_a'] == 'gpt-4':\n",
    "        for text_pair in zip(line['conversation_a'], line['conversation_b']):\n",
    "            generation_pairs[line['model_b']].append((text_pair[0], text_pair[1]))\n",
    "    elif line['model_b'] == 'gpt-4':\n",
    "        for text_pair in zip(line['conversation_b'], line['conversation_a']):\n",
    "            generation_pairs[line['model_a']].append((text_pair[0], text_pair[1]))\n",
    "\n",
    "llm = LLM(\n",
    "    est_path,\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=2048,\n",
    "    tensor_parallel_size=torch.cuda.device_count()\n",
    ")\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0, \n",
    "    prompt_logprobs=0, \n",
    "    max_tokens=1\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(est_path)\n",
    "\n",
    "def mauve_pipeline(human_data, model_data):\n",
    "    predictions = [x['input'] + x['output'] for x in model_data]\n",
    "    references = [x['input'] + x['output'] for x in human_data]\n",
    "    return mauve.compute_mauve(p_text=predictions, q_text=references, featurize_model_name=est_path, verbose=False)\n",
    "\n",
    "mauve_scores = {}\n",
    "\n",
    "for model_name in tqdm(models):\n",
    "    input = []\n",
    "    output = []\n",
    "    for text_pair in generation_pairs[model_name]:\n",
    "        if text_pair[1]['role'] == 'user':\n",
    "            input.append(text_pair[1]['content'])\n",
    "        else:\n",
    "            output.append(text_pair[1]['content'])\n",
    "    data = pa.Table.from_pydict(\n",
    "        dict(\n",
    "            zip(schema.names, (input, output))\n",
    "        ),\n",
    "        schema=schema\n",
    "    )\n",
    "    data = Dataset(data)\n",
    "    model_data = data.filter(lambda x: x['input'] != '' and x['output'] != '')\n",
    "\n",
    "    input = []\n",
    "    output = []\n",
    "    for text_pair in generation_pairs[model_name]:\n",
    "        if text_pair[0]['role'] == 'user':\n",
    "            input.append(text_pair[0]['content'])\n",
    "        else:\n",
    "            output.append(text_pair[0]['content'])\n",
    "    data = pa.Table.from_pydict(\n",
    "        dict(\n",
    "            zip(schema.names, (input, output))\n",
    "        ),\n",
    "        schema=schema\n",
    "    )\n",
    "    data = Dataset(data)\n",
    "    human_data = data.filter(lambda x: x['input'] != '' and x['output'] != '')\n",
    "    mauve_scores[model_name] = mauve_pipeline(human_data, model_data)\n",
    "\n",
    "print(mauve_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT-Bench Zipf Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "\n",
    "est_path = '/data1/model/pythia-1_4b-base'\n",
    "inference_model_name = est_path.split('/')[-1]\n",
    "gpu_mem = 0.9\n",
    "\n",
    "schema = pa.schema({\n",
    "    'input': pa.string(),\n",
    "    'output': pa.string()\n",
    "})\n",
    "# model_generations = {model: set() for model in models}\n",
    "# for line in mt_bench['train']:\n",
    "#     for content in line['conversation_a']:\n",
    "#         model_generations[line['model_a']].add(content['content'])\n",
    "#     for content in line['conversation_b']:\n",
    "#         model_generations[line['model_b']].add(content['content'])\n",
    "\n",
    "generation_pairs = {model: [] for model in models}\n",
    "for line in mt_bench['train']:\n",
    "    if line['model_a'] == 'gpt-4':\n",
    "        for text_pair in zip(line['conversation_a'], line['conversation_b']):\n",
    "            generation_pairs[line['model_b']].append((text_pair[0], text_pair[1]))\n",
    "    elif line['model_b'] == 'gpt-4':\n",
    "        for text_pair in zip(line['conversation_b'], line['conversation_a']):\n",
    "            generation_pairs[line['model_a']].append((text_pair[0], text_pair[1]))\n",
    "\n",
    "# Zipf dict\n",
    "zipf_dict = {}\n",
    "for model_name in models:\n",
    "    for text_pair in generation_pairs[model_name]:\n",
    "        if text_pair[1]['role'] != 'user':\n",
    "            text = text_pair[1]['content']\n",
    "            words = text.split()\n",
    "            words = [word.lower() for word in words]\n",
    "            words = [''.join(x for x in word if x.isalpha()) for word in words]\n",
    "            for word in words:\n",
    "                if word in zipf_dict.keys():\n",
    "                    zipf_dict[word] += 1\n",
    "                else:\n",
    "                    zipf_dict[word] = 1\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# zipf_dict = dict(sorted(zipf_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "# x = list(zipf_dict.keys())  [:50]\n",
    "# y = list(zipf_dict.values())[:50]\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.plot(x, y)\n",
    "# plt.show()\n",
    "\n",
    "total_freq = sum(zipf_dict.values())\n",
    "\n",
    "zipf_scores = {}\n",
    "for model_name in models:\n",
    "    total_prob = 0\n",
    "    cnt = 0\n",
    "    for text_pair in generation_pairs[model_name]:\n",
    "        if text_pair[1]['role'] != 'user':\n",
    "            text = text_pair[1]['content']\n",
    "            words = text.split()\n",
    "            words = [word.lower() for word in words]\n",
    "            words = [''.join(x for x in word if x.isalpha()) for word in words]\n",
    "            for word in words:\n",
    "                total_prob += zipf_dict[word] / total_freq\n",
    "                cnt += 1\n",
    "    if cnt == 0:\n",
    "        continue\n",
    "    zipf_scores[model_name] = total_prob / cnt\n",
    "\n",
    "zipf_scores = dict(sorted(zipf_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "x = list(zipf_scores.keys())  [:50]\n",
    "y = list(zipf_scores.values())[:50]\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(x, y)\n",
    "plt.show()\n",
    "print(zipf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Scores BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import transformers\n",
    "# transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "# transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "# transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "import bert_score\n",
    "bert_score.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "from bert_score import score\n",
    "bert_win_count = {model_a: {model_b: 0 for model_b in models} for model_a in models}\n",
    "\n",
    "schema = pa.schema({\n",
    "    'input': pa.string(),\n",
    "    'output': pa.string()\n",
    "})\n",
    "\n",
    "def bert_pipeline(human_data, model_data):\n",
    "    predictions = [x['output'] for x in model_data]\n",
    "    references = [x['output'] for x in human_data]\n",
    "    P, R, F1 = score(cands=predictions, refs=references, lang='en', verbose=False)\n",
    "    return F1.tolist()\n",
    "\n",
    "gpt4_dict = {}\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    if line['model_a'] == 'gpt-4':\n",
    "        for text in line['conversation_a']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "    elif line['model_b'] == 'gpt-4':\n",
    "        for text in line['conversation_b']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    def eval_line(conversation_name):\n",
    "        input = []\n",
    "        output = []\n",
    "        gpt4_output = []\n",
    "        for text in line[conversation_name]:\n",
    "            if text['role'] == 'user':\n",
    "                text_input = text['content']\n",
    "            else:\n",
    "                if text_input != \"\" and text['content'] != \"\":\n",
    "                    input.append(text_input)\n",
    "                    output.append(text['content'])\n",
    "                    gpt4_output.append(gpt4_dict[text_input][0])\n",
    "        model_data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        model_data = Dataset(model_data)\n",
    "\n",
    "        human_data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, gpt4_output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        human_data = Dataset(human_data)\n",
    "        raw_results = bert_pipeline(human_data, model_data)\n",
    "\n",
    "        return np.mean(raw_results)\n",
    "\n",
    "    result_a = eval_line('conversation_a')\n",
    "    result_b = eval_line('conversation_b')\n",
    "\n",
    "    mean_result_a = result_a\n",
    "    mean_result_b = result_b\n",
    "\n",
    "    if mean_result_a > mean_result_b:\n",
    "        bert_win_count[line['model_b']][line['model_a']] += 1\n",
    "    elif mean_result_a < mean_result_b:\n",
    "        bert_win_count[line['model_a']][line['model_b']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bert score\")\n",
    "print('-' * 10)\n",
    "df = pd.DataFrame(bert_win_count)\n",
    "p, estimates = iterate(df, n=100)\n",
    "print(p)\n",
    "print('=' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bart Scores BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "from bart_score import BARTScorer\n",
    "bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pyarrow as pa\n",
    "bart_win_count = {model_a: {model_b: 0 for model_b in models} for model_a in models}\n",
    "\n",
    "schema = pa.schema({\n",
    "    'input': pa.string(),\n",
    "    'output': pa.string()\n",
    "})\n",
    "\n",
    "def bart_pipeline(human_data, model_data):\n",
    "    predictions = [x['output'] for x in model_data]\n",
    "    references = [x['output'] for x in human_data]\n",
    "    F1 = bart_scorer.score(predictions, references)\n",
    "    return F1\n",
    "\n",
    "gpt4_dict = {}\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    if line['model_a'] == 'gpt-4':\n",
    "        for text in line['conversation_a']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "    elif line['model_b'] == 'gpt-4':\n",
    "        for text in line['conversation_b']:\n",
    "            if text['role'] == 'user':\n",
    "                line_input = text['content']\n",
    "            else:\n",
    "                if line_input in gpt4_dict.keys():\n",
    "                    gpt4_dict[line_input].append(text['content'])\n",
    "                else:\n",
    "                    gpt4_dict[line_input] = [text['content']]\n",
    "\n",
    "for line in tqdm(mt_bench['train']):\n",
    "    def eval_line(conversation_name):\n",
    "        input = []\n",
    "        output = []\n",
    "        gpt4_output = []\n",
    "        for text in line[conversation_name]:\n",
    "            if text['role'] == 'user':\n",
    "                text_input = text['content']\n",
    "            else:\n",
    "                if text_input != \"\" and text['content'] != \"\":\n",
    "                    input.append(text_input)\n",
    "                    output.append(text['content'])\n",
    "                    gpt4_output.append(gpt4_dict[text_input][0])\n",
    "        model_data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        model_data = Dataset(model_data)\n",
    "\n",
    "        human_data = pa.Table.from_pydict(\n",
    "            dict(\n",
    "                zip(schema.names, (input, gpt4_output))\n",
    "            ),\n",
    "            schema=schema\n",
    "        )\n",
    "        human_data = Dataset(human_data)\n",
    "        raw_results = bart_pipeline(human_data, model_data)\n",
    "\n",
    "        return np.mean(raw_results)\n",
    "\n",
    "    result_a = eval_line('conversation_a')\n",
    "    result_b = eval_line('conversation_b')\n",
    "\n",
    "    mean_result_a = result_a\n",
    "    mean_result_b = result_b\n",
    "\n",
    "    if mean_result_a > mean_result_b:\n",
    "        bart_win_count[line['model_b']][line['model_a']] += 1\n",
    "    elif mean_result_a < mean_result_b:\n",
    "        bart_win_count[line['model_a']][line['model_b']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bart score\")\n",
    "print('-' * 10)\n",
    "df = pd.DataFrame(bart_win_count)\n",
    "p, estimates = iterate(df, n=100)\n",
    "print(p)\n",
    "print('=' * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
